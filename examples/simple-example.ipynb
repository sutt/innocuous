{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049afbeb",
   "metadata": {},
   "source": [
    "### Example of using `Innocuous` package\n",
    "_How to use it in a script or notebook_ | _August 26, 2025 |  `v0.2.0`_\n",
    "\n",
    "We'll demonstrate how to:\n",
    "- Check the setup.\n",
    "- Encode some data into a text generation.\n",
    "- Decode that text back to the data.\n",
    "\n",
    "For package install instructions, see [README](../README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e91098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stego_llm import main_decode, main_encode\n",
    "from stego_llm.llm.interface import check_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a214a232",
   "metadata": {},
   "source": [
    "### Check Setup\n",
    "\n",
    "Let's check that the package can read and load the LLM weights we have locally, we'll pass the path the **gguf** file to `check_llm` method.\n",
    "\n",
    "The below is equivalent to cli: \n",
    "`innocuous --llm-path path/to/model.gguf check-llm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5163f01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LLM path from argument: /home/user/dev/innocuous/data/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "LLM path set to: /home/user/dev/innocuous/data/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "LLM file found at: /home/user/dev/innocuous/data/mistral-7b-instruct-v0.2.Q4_K_M.gguf\n",
      "Attempting to load LLM...\n",
      "LLM loaded successfully.\n",
      "Performing simple inference task...\n",
      "Inference task successful.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_path = \"/home/user/dev/innocuous/data/mistral-7b-instruct-v0.2.Q4_K_M.gguf\"\n",
    "check_llm(llm_path=llm_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dff88ff",
   "metadata": {},
   "source": [
    "### Encode Data to Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d878b26a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an iambic penatameter poem. Complete it:\n",
      "The king with a frown and a furrowed brow,\n",
      "Looked upon his realm in despair and sorrow,\n",
      "The harvest was scanty, the skies clouded now,\n",
      "The crops withered and starved the people below,\n",
      "\n",
      "What steps should the king take to save\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "message_to_encode = \"marry me\"\n",
    "initial_prompt = \"Below is an iambic penatameter poem. Complete it:\\nThe king\"\n",
    "chunk_size = 2   # best values are 2,3,4\n",
    "num_logprobs = 100  # not very important, but if you hit error, you can increase this.\n",
    "\n",
    "# run\n",
    "msg = message_to_encode.encode('utf-8') # must encode to bytes before passing in\n",
    "result = main_encode(\n",
    "    initial_prompt=initial_prompt,\n",
    "    msg=msg,\n",
    "    chunk_size=chunk_size,\n",
    "    num_logprobs=num_logprobs,\n",
    "    llm_path=llm_path,\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77289df",
   "metadata": {},
   "source": [
    "### Analysis on Text Generation\n",
    "We can see printing `result` includes the `initial_prompt`. Let's do some processing steps to see how much text was generated for this message.\n",
    "\n",
    "We can see for an:\n",
    "- encoded message of 8 characters / 2 words, \n",
    "- we produce 216 characters / 39 words of text generation.\n",
    "\n",
    "To cut down this ratio, we can increase the `chunk_size` parameter in the step above to 3 or 4.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5903d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### generated text with inital_prompt removed:\n",
      " with a frown and a furrowed brow,\n",
      "Looked upon his realm in despair and sorrow,\n",
      "The harvest was scanty, the skies clouded now,\n",
      "The crops withered and starved the people below,\n",
      "\n",
      "What steps should the king take to save\n",
      "\n",
      "### stats:\n",
      "num chars: 216\n",
      "num words: 39\n"
     ]
    }
   ],
   "source": [
    "just_generation = result[len(initial_prompt) : ]\n",
    "print(\"### generated text with inital_prompt removed:\")\n",
    "print(just_generation)\n",
    "print(\"\\n### stats:\")\n",
    "print(f\"num chars: {len(just_generation)}\")\n",
    "print(f\"num words: {len(just_generation.split())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57c01b",
   "metadata": {},
   "source": [
    "### Decode Text Generation to Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ac9973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params - these were already defined above, so we leave them commented out\n",
    "# initial_prompt = \"Below is an iambic penatameter poem. Complete it:\\nThe king\"\n",
    "# chunk_size = 2\n",
    "# num_logprobs = 100\n",
    "\n",
    "# run\n",
    "recovered_data = main_decode(\n",
    "    encoded_prompt=result,\n",
    "    initial_prompt=initial_prompt,\n",
    "    chunk_size=chunk_size,\n",
    "    num_logprobs=num_logprobs,\n",
    "    llm_path=llm_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f042d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OriginalMessage: marry me\n",
      "RecoveredMessage: b'marry me'\n",
      "it's a match :)\n"
     ]
    }
   ],
   "source": [
    "print(f\"OriginalMessage: {message_to_encode}\")\n",
    "print(f\"RecoveredMessage: {recovered_data}\")\n",
    "\n",
    "if recovered_data.decode('utf-8') == message_to_encode:\n",
    "    print(\"it's a match :)\")\n",
    "else:\n",
    "    print(\"they don't match :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc971e4",
   "metadata": {},
   "source": [
    "Notice that the recovered message comes out as utf-8 byte mode. So we'll decode it to match the `str` data type of our original message. \n",
    "\n",
    "And the contents of the original-message and recovered-message **match**.\n",
    "\n",
    "This is the end of the simple tutorial: _Have fun with it, Be creative, and Stay Innocuous my friends_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo-llama-uv",
   "language": "python",
   "name": "demo-llama-uv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
